{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLXMHYyvrz8KOBHXs3V3Ht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahidhamidi/ML-Project/blob/main/Rule_based_Method_Experimentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Unit Test for Text preprocessing**"
      ],
      "metadata": {
        "id": "bPwPZAKnb9TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJAFq2JddCa7",
        "outputId": "48f49735-4f09-414e-c92f-f0f0c3efd3af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "chwanb7Zb3qa"
      },
      "outputs": [],
      "source": [
        "def unit_test(input_text):\n",
        "\n",
        "  # Define the topics as flexible terms (subwords or whole words)\n",
        "  topics = [\"overpressure\", \"sand production\", \"oil show\"]\n",
        "\n",
        "  # Define the negation terms pattern with case-insensitivity\n",
        "  negation_terms = r'(?i)(?:(?<=\\s)|^)(no|none|not|nor|without)(?=\\s|$)'\n",
        "\n",
        "  # Initialize a WordNet lemmatizer\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  # Define a custom lemmatization rule to replace \"ized\" with \"e\"\n",
        "  def custom_lemmatizer(word):\n",
        "      if word.endswith(\"ized\"):\n",
        "          return re.sub(r'ized$', 'e', word)\n",
        "      else:\n",
        "          return lemmatizer.lemmatize(word)\n",
        "\n",
        "  # Initialize a list to store selected sentences\n",
        "  selected_sentences_1 = []\n",
        "\n",
        "  # Convert the text to lowercase\n",
        "  text = input_text.lower()  # Convert to lowercase\n",
        "\n",
        "  # Tokenize the text into sentences using NLTK's sent_tokenize\n",
        "  sentences = sent_tokenize(text)\n",
        "\n",
        "  # Initialize a flag to check for both topic terms and negation terms\n",
        "  both_detected = False\n",
        "\n",
        "\n",
        "  if len(sentences) == 1:\n",
        "      # If there is only one sentence, append the entire text\n",
        "\n",
        "      selected_sentences_1.append(text)\n",
        "  else:\n",
        "      for sentence in sentences:\n",
        "\n",
        "\n",
        "        # Initialize a flag to check if any topic term is found in the sentence\n",
        "        topic_found = False\n",
        "\n",
        "        # Tokenize the sentence into words\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Lemmatize each word using the custom lemmatizer\n",
        "        lemmatized_words = [custom_lemmatizer(word) for word in words]\n",
        "\n",
        "        # Join the lemmatized words back into a sentence\n",
        "        lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "        # Check if the sentence contains any of the specified topics (partial matches)\n",
        "        for topic in topics:\n",
        "            if topic in lemmatized_sentence.lower():  # Convert to lowercase for matching\n",
        "                topic_found = True\n",
        "                break\n",
        "\n",
        "        # Check if the lemmatized sentence contains negation terms\n",
        "        if re.search(negation_terms, lemmatized_sentence.lower()):\n",
        "            negation_detected = True\n",
        "        else:\n",
        "            negation_detected = False\n",
        "\n",
        "        # If a topic term is found and either no negation terms are found or negation_detected is False, append the sentence\n",
        "        if (topic_found is True) and (negation_detected is True):\n",
        "            selected_sentences_1.append(lemmatized_sentence)\n",
        "            both_detected = True\n",
        "\n",
        "\n",
        "        elif (topic_found is True) and (negation_detected is False):\n",
        "            selected_sentences_1.append(lemmatized_sentence)\n",
        "            both_detected = True\n",
        "\n",
        "\n",
        "\n",
        "      # If no suitable sentence is found, set 'None' as a placeholder\n",
        "      if not both_detected:\n",
        "          selected_sentences_1.append(None)\n",
        "\n",
        "  # Determine the prediction (0 or 1) based on the presence of negation terms in selected_sentence_1\n",
        "  # selected_sentence = selected_sentences_1[0]\n",
        "\n",
        "  neg = [\"no\", \"not\", \"lack\", \"nor\" , \"none\" , \"without\", \"rather than\"]\n",
        "\n",
        "  if len(selected_sentences_1) != 1:\n",
        "    for text in selected_sentences_1:\n",
        "      for items in neg:\n",
        "        if items in text:\n",
        "          selected_sentence = text\n",
        "\n",
        "  else:\n",
        "    selected_sentence = selected_sentences_1\n",
        "    selected_sentence = selected_sentence[0]\n",
        "\n",
        "\n",
        "\n",
        "  if selected_sentence is not None:\n",
        "      # Check if the lemmatized sentence contains negation terms\n",
        "      if re.search(negation_terms, selected_sentence.lower()):\n",
        "          prediction = 0  # Negation terms found, set the prediction to 0\n",
        "      else:\n",
        "          prediction = 1  # No negation terms found, set the prediction to 1\n",
        "  else:\n",
        "      prediction = 0  # Handle the case where selected_sentence is None\n",
        "\n",
        "  # Print the selected sentence and prediction\n",
        "  print(\"Selected Sentence:\", selected_sentence)\n",
        "  print(\"Prediction:\", prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your CSV dataset\n",
        "df = pd.read_csv(\"true_positive_experimentation.csv\")"
      ],
      "metadata": {
        "id": "nhdQruBlcLPA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "input_text = \"If abnormally compacted variations are penetrated, the resulting data points diverge from the ”normal compaction trend”. In case of overpressures, that is uncompacted interval, +the shale transit times ( 4 t shale) will be higher for a given depth, since porosity is higher. The amount of divergence of a given point from the established ”normal compaction trend” has been '\"\n",
        "unit_test(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P9t5toAcEH2",
        "outputId": "a65fa7ea-0ffa-4253-aa24-653a87b13b2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Sentence: in case of overpressure , that is uncompacted interval , +the shale transit time ( 4 t shale ) will be higher for a given depth , since porosity is higher .\n",
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNrLJ5hucG47"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
