{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZhieoyGnrHzaZUlQi1ylo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahidhamidi/ML-Project/blob/main/Rule_based_method_dataframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyAXc5rsBZCc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tkinter.constants import NONE\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/sample_data/robust_test_dataset.xlsx\")\n",
        "# df_stuck_pipe = df[df[\"dataset\"] == \"stuck pipe\"]\n",
        "# df_pbu = df[df[\"dataset\"] == \"pressure build-up\"]\n",
        "# df_gas_shows = df[df[\"dataset\"] == \"gas shows\"]\n",
        "# df_gbearing = df[df[\"dataset\"] == \"gas bearing\"]\n",
        "# df_fc = df[df[\"dataset\"] == \"fluid communication\"]\n",
        "# df_op = df[df[\"dataset\"] == \"overpressure\"]\n",
        "df_oil_shows = df[df[\"dataset\"] == \"oil show\"]\n",
        "# df_sand = df[df[\"dataset\"] == \"sand_production\"]\n",
        "\n",
        "df = df_oil_shows"
      ],
      "metadata": {
        "id": "w76B7Nb1BdU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the topics as flexible terms (subwords or whole words)\n",
        "# topics = [\"gas show\" , \"oil show\", \"overpressure\", \"fluid communication\" , \"gas bearing\" , \"h2s\" , \"co2\" , \"stuck pipe\" , \"pressure build up\", \"sand production\"]\n",
        "topics = [\"oil shows\"]\n",
        "\n",
        "# Define the negation terms pattern with case-insensitivity\n",
        "negation_terms = r'(?i)(?:(?<=\\s)|^)(no|non|unlikely|none|not|nor|without|lack|rather)(?=\\s|$|\\b)'\n",
        "\n",
        "# Initialize a WordNet lemmatizer\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a custom lemmatization rule to replace \"ized\" with \"e\"\n",
        "def custom_lemmatizer(word):\n",
        "    if word.endswith(\"ized\"):\n",
        "        return re.sub(r'ized$', 'e', word)\n",
        "    elif word.endswith(\"ing\"):\n",
        "        return re.sub(r'ing$', 'e', word)\n",
        "    else:\n",
        "        return word\n",
        "        # return lemmatizer.lemmatize(word)\n",
        "\n",
        "# Initialize a list to store selected sentences\n",
        "selected_sentences = []\n",
        "\n",
        "# Initialize a flag to check for both topic terms and negation terms\n",
        "both_detected = False\n",
        "\n",
        "# Iterate over the 'date' column and apply .lower() only to string values\n",
        "for index, row in df.iterrows():\n",
        "    if isinstance(row['doc_text_original'], str):\n",
        "        # Apply .lower() to string values\n",
        "        text = row['doc_text_original'].lower()  # Convert to lowercase\n",
        "\n",
        "    # Tokenize the text into sentences using NLTK's sent_tokenize\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize flags to check for both topic terms and negation terms\n",
        "    topic_found = False\n",
        "    negation_detected = False\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # remove apostrophes from string\n",
        "      sentence = sentence.replace(\"'\", \"\")\n",
        "\n",
        "      # Tokenize the sentence into words\n",
        "      words = word_tokenize(sentence)\n",
        "\n",
        "      # Lemmatize each word using the custom lemmatizer\n",
        "      lemmatized_words = [custom_lemmatizer(word) for word in words]\n",
        "\n",
        "      # Join the lemmatized words back into a sentence\n",
        "      lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "      # Check if the sentence contains any of the specified topics (partial matches)\n",
        "      for topic_pattern in topics:\n",
        "        words = topic_pattern.split()\n",
        "\n",
        "      if len(words) >= 2:\n",
        "        # Initialize an empty pattern string\n",
        "        pattern_string = r'(?i)(?:(?<=\\s)|^)'\n",
        "\n",
        "        # Loop through the words and add them to the pattern\n",
        "        for i in range(len(words)):\n",
        "            pattern_string += re.escape(words[i])\n",
        "\n",
        "            # Add optional characters (like hyphens) between words (except for the last word)\n",
        "            if i < len(words) - 1:\n",
        "                pattern_string += r'\\s*-*\\s*'\n",
        "\n",
        "        # Add the closing part of the pattern\n",
        "        pattern_string += r'(?=\\s|$)'\n",
        "\n",
        "        # Compile the regex pattern\n",
        "        topic_pattern = re.compile(pattern_string)\n",
        "\n",
        "        if re.search(topic_pattern, lemmatized_sentence.lower()):\n",
        "            topic_found = True\n",
        "            break\n",
        "\n",
        "        \"\"\"\n",
        "        if len(words) >= 2:\n",
        "          # Define the regex pattern for two-word topics with optional characters in the middle\n",
        "          two_word_topic_pattern = re.compile(rf'(?i)(?:(?<=\\s)|^)({re.escape(words[0])}\\s*-*\\s*{re.escape(words[1])})(?=\\s|$)')\n",
        "          if re.search(two_word_topic_pattern, lemmatized_sentence.lower()):\n",
        "            topic_found = True\n",
        "            break\n",
        "        \"\"\"\n",
        "\n",
        "      else:\n",
        "          single_pattern = rf'(?i)(?:(?<=\\s)|^)({re.escape(words[0])})(?=\\s|$)'\n",
        "          if re.search(single_pattern, lemmatized_sentence.lower()):\n",
        "              topic_found = True\n",
        "              break\n",
        "\n",
        "      # Check if the lemmatized sentence contains negation terms\n",
        "      if re.search(negation_terms, lemmatized_sentence.lower()):\n",
        "          negation_detected = True\n",
        "      else:\n",
        "          negation_detected = False\n",
        "\n",
        "      # If a topic term is found and either no negation terms are found or negation_detected is False, append the sentence\n",
        "      if (topic_found is True) and (negation_detected is True):\n",
        "          selected_sentences.append(lemmatized_sentence)\n",
        "          both_detected = True\n",
        "          break\n",
        "\n",
        "      elif (topic_found is True) and (negation_detected is False):\n",
        "          selected_sentences.append(lemmatized_sentence)\n",
        "          both_detected = True\n",
        "          break\n",
        "\n",
        "\n",
        "    # If no sentence with both topic and negation terms is found, append the first sentence without a negation term\n",
        "    if not both_detected or topic_found is False:\n",
        "        selected_sentences.append(lemmatized_sentence)\n",
        "\n",
        "\n",
        "\n",
        " # Add the selected_sentences as a new column named 'selected_sentence'\n",
        "df['selected_sentence'] = selected_sentences\n",
        "\n",
        "print(len(selected_sentences))\n"
      ],
      "metadata": {
        "id": "fHAz3-CJBdkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the prediction (0 or 1) based on the presence of negation terms in selected_sentence\n",
        "for index, row in df.iterrows():\n",
        "    selected_sentence = row['selected_sentence']\n",
        "\n",
        "    if selected_sentence is not None:\n",
        "        # Check if the lemmatized sentence contains negation terms\n",
        "        if re.search(negation_terms, selected_sentence.lower()):\n",
        "            prediction = 0  # Negation terms found, set the prediction to 0\n",
        "        else:\n",
        "            prediction = 1  # No negation terms found, set the prediction to 1\n",
        "    else:\n",
        "        prediction = 0  # Handle the case where selected_sentence is None\n",
        "\n",
        "    df.at[index, 'predicted'] = prediction\n",
        "\n",
        "# Print the DataFrame with the 'label' column\n",
        "df[['doc_text_original', 'selected_sentence', 'label_code', 'predicted']]"
      ],
      "metadata": {
        "id": "UlSenAG2Bkkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Negation Prediction with SpaCy Negex package**"
      ],
      "metadata": {
        "id": "aqyWZUxE1Nt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "2HmCivh51FEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library for sentiment analysis\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Initialize a list to store predictions (0 for negation, 1 for non-negation)\n",
        "predictions_negex = []\n",
        "\n",
        "# Iterate through each row of the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    selected_sentence = row['selected_sentence']\n",
        "\n",
        "    if selected_sentence is not None:\n",
        "        # Use SentimentIntensityAnalyzer to get sentiment polarity score\n",
        "        sentiment_score = sia.polarity_scores(selected_sentence)\n",
        "\n",
        "        # Determine the prediction based on the sentiment score\n",
        "        if sentiment_score['compound'] < 0:\n",
        "            prediction_negex = 0  # Negative sentiment, set prediction to 0 (negation)\n",
        "        else:\n",
        "            prediction_negex = 1  # Positive or neutral sentiment, set prediction to 1 (non-negation)\n",
        "    else:\n",
        "        prediction_negex = 0  # Handle the case where selected_sentence is None\n",
        "\n",
        "    predictions_negex.append(prediction_negex)\n",
        "\n",
        "# Add the predictions_negex as a new column named 'prediction_negex'\n",
        "df['prediction_negex'] = predictions_negex\n",
        "\n",
        "# Print the DataFrame with the 'label' column and the new 'prediction_negex' column\n",
        "# df[['doc_text_original', 'selected_sentence', 'label_code', 'predicted', 'prediction_negex']]\n"
      ],
      "metadata": {
        "id": "__v5y4Di1USS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "681C_Fx72JoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction Evaluation**"
      ],
      "metadata": {
        "id": "1EeLApkmDaAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy_values = []\n",
        "precision_values = []\n",
        "recall_values = []\n",
        "f1_values = []\n",
        ""
      ],
      "metadata": {
        "id": "UuCHHu9I1Lmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have loaded your DataFrame 'df' with the columns 'label_code' and 'predicted_label_code'\n",
        "y_true = df['label_code']\n",
        "y_pred = df['prediction_negex']\n",
        "# y_pred = df['predicted']\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate additional evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Append metric values to the respective lists\n",
        "accuracy_values.append(accuracy)\n",
        "precision_values.append(precision)\n",
        "recall_values.append(recall)\n",
        "f1_values.append(f1)\n",
        "\n",
        "print(accuracy, precision, recall, f1)"
      ],
      "metadata": {
        "id": "fse5OQWiBtri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a box plot for the accumulated evaluation metrics\n",
        "metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "metric_values = [accuracy_values, precision_values, recall_values, f1_values]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=metric_values, orient=\"v\", palette=\"Set3\")\n",
        "sns.stripplot(data=metric_values, orient=\"v\", palette=\"Set1\", size=4, jitter=True)\n",
        "\n",
        "# Add legends\n",
        "legends = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=5, label='Data Points')]\n",
        "plt.legend(handles=legends, loc='upper right')\n",
        "plt.ylim(0, 1)  # Set the y-axis range from 0 to 1\n",
        "\n",
        "plt.xticks(range(4), metric_names)\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Box Plots for Accumulated Evaluation Metrics with Data Points\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PlguWUnHbY9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "df.to_excel(\"dataset_gb.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "3uTPjWzZevvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1viUzV69A4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVdveMEJ9BWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}