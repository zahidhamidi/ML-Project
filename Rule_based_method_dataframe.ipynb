{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeDaLynXqy9qf0Jx275jOQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahidhamidi/ML-Project/blob/main/Rule_based_method_dataframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lyAXc5rsBZCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa36226-3ace-4116-91b8-0ebb5e230c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tkinter.constants import NONE\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/sample_data/robust_test_dataset.xlsx\")\n",
        "# df_stuck_pipe = df[df[\"dataset\"] == \"stuck pipe\"]\n",
        "# df_pbu = df[df[\"dataset\"] == \"pressure build-up\"]\n",
        "# df_gas_shows = df[df[\"dataset\"] == \"gas shows\"]\n",
        "# df_gbearing = df[df[\"dataset\"] == \"gas bearing\"]\n",
        "# df_fc = df[df[\"dataset\"] == \"fluid communication\"]\n",
        "# df_op = df[df[\"dataset\"] == \"overpressure\"]\n",
        "# df_oil_shows = df[df[\"dataset\"] == \"oil show\"]\n",
        "# df_sand = df[df[\"dataset\"] == \"sand_production\"]\n",
        "\n",
        "# df = df_oil_shows"
      ],
      "metadata": {
        "id": "w76B7Nb1BdU3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the topics as flexible terms (subwords or whole words)\n",
        "topics = [\"gas shows\" , \"oil show\", \"overpressure\", \"fluid communication\" , \"gas bearing\" , \"h2s\" , \"co2\" , \"stuck pipe\" , \"pressure build up\", \"sand production\"]\n",
        "# topics = [\"oil show\"]\n",
        "\n",
        "# Replace hyphens with blank spaces in all items in the topics list\n",
        "topics = [topic.replace('-', ' ') for topic in topics]\n",
        "\n",
        "# Define the negation terms pattern with case-insensitivity\n",
        "negation_terms = r'(?i)(?:(?<=\\s)|^)(no|non|unlikely|none|not|nor|without|lack|rather)(?=\\s|$|\\b)'\n",
        "\n",
        "# Initialize a WordNet lemmatizer\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a custom lemmatization rule to replace \"ized\" with \"e\"\n",
        "def custom_lemmatizer(word):\n",
        "    if word.endswith(\"ized\"):\n",
        "        return re.sub(r'ized$', 'e', word)\n",
        "    elif word.endswith(\"ing\"):\n",
        "        return re.sub(r'ing$', 'e', word)\n",
        "    else:\n",
        "        return word\n",
        "        # return lemmatizer.lemmatize(word)\n",
        "\n",
        "# Initialize a list to store selected sentences\n",
        "selected_sentences = []\n",
        "\n",
        "# Initialize a flag to check for both topic terms and negation terms\n",
        "both_detected = False\n",
        "\n",
        "# Iterate over the 'date' column and apply .lower() only to string values\n",
        "for index, row in df.iterrows():\n",
        "    if isinstance(row['doc_text_original'], str):\n",
        "        # Apply .lower() to string values\n",
        "        text = row['doc_text_original'].lower()  # Convert to lowercase\n",
        "\n",
        "    # Tokenize the text into sentences using NLTK's sent_tokenize\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize flags to check for both topic terms and negation terms\n",
        "    topic_found = False\n",
        "    negation_detected = False\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # remove apostrophes from string\n",
        "      sentence = sentence.replace(\"'\", \"\")\n",
        "      sentence = sentence.replace(\",\", \" \")\n",
        "\n",
        "      # Tokenize the sentence into words\n",
        "      words = word_tokenize(sentence)\n",
        "\n",
        "      # Lemmatize each word using the custom lemmatizer\n",
        "      # lemmatized_words = [custom_lemmatizer(word) for word in words]\n",
        "\n",
        "      # Join the lemmatized words back into a sentence\n",
        "      lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "      # Check if the sentence contains any of the specified topics (partial matches)\n",
        "      for topic_pattern in topics:\n",
        "        words = topic_pattern.split()\n",
        "\n",
        "      if len(words) >= 2:\n",
        "        # Initialize an empty pattern string\n",
        "        pattern_string = r'(?i)(?:(?<=\\s)|^)'\n",
        "\n",
        "        # Loop through the words and add them to the pattern\n",
        "        for i in range(len(words)):\n",
        "            pattern_string += re.escape(words[i])\n",
        "\n",
        "            # Add optional characters (like hyphens) between words (except for the last word)\n",
        "            if i < len(words) - 1:\n",
        "                pattern_string += r'\\s*-*\\s*'\n",
        "\n",
        "        # Add the closing part of the pattern\n",
        "        pattern_string += r'(?=\\s|$)'\n",
        "\n",
        "        # Compile the regex pattern\n",
        "        topic_pattern = re.compile(pattern_string)\n",
        "\n",
        "        if re.search(topic_pattern, lemmatized_sentence.lower()):\n",
        "            topic_found = True\n",
        "            break\n",
        "\n",
        "        \"\"\"\n",
        "        if len(words) >= 2:\n",
        "          # Define the regex pattern for two-word topics with optional characters in the middle\n",
        "          two_word_topic_pattern = re.compile(rf'(?i)(?:(?<=\\s)|^)({re.escape(words[0])}\\s*-*\\s*{re.escape(words[1])})(?=\\s|$)')\n",
        "          if re.search(two_word_topic_pattern, lemmatized_sentence.lower()):\n",
        "            topic_found = True\n",
        "            break\n",
        "        \"\"\"\n",
        "\n",
        "      else:\n",
        "          single_pattern = rf'(?i)(?:(?<=\\s)|^)({re.escape(words[0])})(?=\\s|$)'\n",
        "          if re.search(single_pattern, lemmatized_sentence.lower()):\n",
        "              topic_found = True\n",
        "              break\n",
        "\n",
        "      # Check if the lemmatized sentence contains negation terms\n",
        "      if re.search(negation_terms, lemmatized_sentence.lower()):\n",
        "          negation_detected = True\n",
        "      else:\n",
        "          negation_detected = False\n",
        "\n",
        "      # If a topic term is found and either no negation terms are found or negation_detected is False, append the sentence\n",
        "      if (topic_found is True) and (negation_detected is True):\n",
        "          selected_sentences.append(lemmatized_sentence)\n",
        "          both_detected = True\n",
        "          break\n",
        "\n",
        "      elif (topic_found is True) and (negation_detected is False):\n",
        "          selected_sentences.append(lemmatized_sentence)\n",
        "          both_detected = True\n",
        "          break\n",
        "\n",
        "\n",
        "    # If no sentence with both topic and negation terms is found, append the first sentence without a negation term\n",
        "    if not both_detected or topic_found is False:\n",
        "        selected_sentences.append(lemmatized_sentence)\n",
        "\n",
        "\n",
        "\n",
        " # Add the selected_sentences as a new column named 'selected_sentence'\n",
        "df['selected_sentence'] = selected_sentences\n",
        "\n",
        "print(len(selected_sentences))\n",
        "df.iloc[1166,-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHAz3-CJBdkD",
        "outputId": "fa317e7a-42c6-48eb-c5f7-438603624f70"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Block"
      ],
      "metadata": {
        "id": "z0Sd1qktgpCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the topics as flexible terms (subwords or whole words)\n",
        "# topics = [\"gas shows\" , \"oil show\", \"overpressure\", \"fluid communication\" , \"gas bearing\" , \"h2s\" , \"co2\" , \"stuck pipe\" , \"pressure build up\", \"sand production\"]\n",
        "topics = [\"gas-bearing\"]\n",
        "\n",
        "# Replace hyphens with blank spaces in all items in the topics list\n",
        "topics = [topic.replace('-', ' ') for topic in topics]\n",
        "\n",
        "# Define the negation terms pattern with case-insensitivity\n",
        "negation_terms = r'(?i)(?:(?<=\\s)|^)(no|non|unlikely|none|not|nor|without|lack|rather)(?=\\s|$|\\b)'\n",
        "\n",
        "# Initialize a WordNet lemmatizer\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a custom lemmatization rule to replace \"ized\" with \"e\"\n",
        "def custom_lemmatizer(word):\n",
        "    if word.endswith(\"ized\"):\n",
        "        return re.sub(r'ized$', 'e', word)\n",
        "    elif word.endswith(\"ing\"):\n",
        "        return re.sub(r'ing$', 'e', word)\n",
        "    else:\n",
        "        return word\n",
        "        # return lemmatizer.lemmatize(word)\n",
        "\n",
        "# Initialize a list to store selected sentences\n",
        "selected_sentences = []\n",
        "\n",
        "# Initialize a flag to check for both topic terms and negation terms\n",
        "both_detected = False\n",
        "\n",
        "\n",
        "text = '535 mMD RKB, as they are correlated to gas bearing sands in other wells. They did not, however, display seismic anomalies at the well site. '\n",
        "# Tokenize the text into sentences using NLTK's sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Initialize flags to check for both topic terms and negation terms\n",
        "topic_found = False\n",
        "negation_detected = False\n",
        "\n",
        "for sentence in sentences:\n",
        "\n",
        "  # remove apostrophes from string\n",
        "  sentence = sentence.replace(\"'\", \"\")\n",
        "  sentence = sentence.replace(\",\", \" \")\n",
        "\n",
        "  # Tokenize the sentence into words\n",
        "  words = word_tokenize(sentence)\n",
        "\n",
        "  # Lemmatize each word using the custom lemmatizer\n",
        "  # lemmatized_words = [custom_lemmatizer(word) for word in words]\n",
        "\n",
        "  # Join the lemmatized words back into a sentence\n",
        "  lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "  lemmatized_sentence = ' '.join(words)\n",
        "\n",
        "  # Check if the sentence contains any of the specified topics (partial matches)\n",
        "\n",
        "\n",
        "  # Initialize an empty pattern string\n",
        "  pattern_string = r'(?i)(?:(?<=\\s)|^)'\n",
        "\n",
        "  for topic_pattern in topics:\n",
        "    words = topic_pattern.split()\n",
        "\n",
        "  if len(words) >= 2:\n",
        "    # Loop through the words\n",
        "    for i in range(len(words)):\n",
        "        # Split each word into individual words and escape them\n",
        "        individual_words = words[i].split()\n",
        "        escaped_words = [re.escape(word) for word in individual_words]\n",
        "\n",
        "\n",
        "        # Create a pattern that matches words with optional hyphens or non-word characters in between\n",
        "        pattern = r'[-\\w\\s]*'.join(escaped_words)\n",
        "\n",
        "\n",
        "        # Append the joined word to the pattern string\n",
        "        pattern_string += joined_word\n",
        "\n",
        "     # Compile the regex pattern\n",
        "    topic_pattern = re.compile(pattern_string)\n",
        "\n",
        "    if re.search(topic_pattern, lemmatized_sentence.lower()):\n",
        "          topic_found = True\n",
        "          break\n",
        "\n",
        "    print(topic_found)\n",
        "    print(lemmatized_sentence)\n",
        "\n",
        "  elif len(words) != 0:\n",
        "      single_pattern = rf'(?i)(?:(?<=\\s)|^)({re.escape(words[0])})(?=\\s|$)'\n",
        "      if re.search(single_pattern, lemmatized_sentence.lower()):\n",
        "          topic_found = True\n",
        "          break\n",
        "\n",
        "  # Check if the lemmatized sentence contains negation terms\n",
        "  if re.search(negation_terms, lemmatized_sentence.lower()):\n",
        "      negation_detected = True\n",
        "  else:\n",
        "      negation_detected = False\n",
        "\n",
        "  # If a topic term is found and either no negation terms are found or negation_detected is False, append the sentence\n",
        "  if (topic_found is True) and (negation_detected is True):\n",
        "      selected_sentences.append(lemmatized_sentence)\n",
        "      both_detected = True\n",
        "      break\n",
        "\n",
        "  elif (topic_found is True) and (negation_detected is False):\n",
        "      selected_sentences.append(lemmatized_sentence)\n",
        "      both_detected = True\n",
        "      break\n",
        "\n",
        "\n",
        "  # If no sentence with both topic and negation terms is found, append the first sentence without a negation term\n",
        "  if not both_detected or topic_found is False:\n",
        "      selected_sentences.append(lemmatized_sentence)\n",
        "\n",
        "print(selected_sentences)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hoMdx-wybQY-",
        "outputId": "e1edf440-26d8-4ad6-9356-a7b7a984edb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "535 mMD RKB as they are correlated to gas bearing sands in other wells .\n",
            "False\n",
            "They did not however display seismic anomalies at the well site .\n",
            "['535 mMD RKB as they are correlated to gas bearing sands in other wells .', 'They did not however display seismic anomalies at the well site .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **End of testing block**"
      ],
      "metadata": {
        "id": "KTgj4cMshVLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the prediction (0 or 1) based on the presence of negation terms in selected_sentence\n",
        "for index, row in df.iterrows():\n",
        "    selected_sentence = row['selected_sentence']\n",
        "\n",
        "    if selected_sentence is not None:\n",
        "        # Check if the lemmatized sentence contains negation terms\n",
        "        if re.search(negation_terms, selected_sentence.lower()):\n",
        "            prediction = 0  # Negation terms found, set the prediction to 0\n",
        "        else:\n",
        "            prediction = 1  # No negation terms found, set the prediction to 1\n",
        "    else:\n",
        "        prediction = 0  # Handle the case where selected_sentence is None\n",
        "\n",
        "    df.at[index, 'predicted'] = prediction\n",
        "\n",
        "# Print the DataFrame with the 'label' column\n",
        "df[['doc_text_original', 'selected_sentence', 'label_code', 'predicted']]"
      ],
      "metadata": {
        "id": "UlSenAG2Bkkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Negation Prediction with SpaCy Negex package**"
      ],
      "metadata": {
        "id": "aqyWZUxE1Nt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "2HmCivh51FEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library for sentiment analysis\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Initialize a list to store predictions (0 for negation, 1 for non-negation)\n",
        "predictions_negex = []\n",
        "\n",
        "# Iterate through each row of the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    selected_sentence = row['selected_sentence']\n",
        "\n",
        "    if selected_sentence is not None:\n",
        "        # Use SentimentIntensityAnalyzer to get sentiment polarity score\n",
        "        sentiment_score = sia.polarity_scores(selected_sentence)\n",
        "\n",
        "        # Determine the prediction based on the sentiment score\n",
        "        if sentiment_score['compound'] < 0:\n",
        "            prediction_negex = 0  # Negative sentiment, set prediction to 0 (negation)\n",
        "        else:\n",
        "            prediction_negex = 1  # Positive or neutral sentiment, set prediction to 1 (non-negation)\n",
        "    else:\n",
        "        prediction_negex = 0  # Handle the case where selected_sentence is None\n",
        "\n",
        "    predictions_negex.append(prediction_negex)\n",
        "\n",
        "# Add the predictions_negex as a new column named 'prediction_negex'\n",
        "df['prediction_negex'] = predictions_negex\n",
        "\n",
        "# Print the DataFrame with the 'label' column and the new 'prediction_negex' column\n",
        "# df[['doc_text_original', 'selected_sentence', 'label_code', 'predicted', 'prediction_negex']]\n"
      ],
      "metadata": {
        "id": "__v5y4Di1USS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "681C_Fx72JoD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction Evaluation**"
      ],
      "metadata": {
        "id": "1EeLApkmDaAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy_values = [0.26,0.23,0.89,0.71,0.69,0.71,0.66]\n",
        "precision_values = [0.48,0.55,0.70,0.52,0.64,0.72,0.55]\n",
        "recall_values = [0.46,0.52,0.94,0.63,0.65,0.76,0.61]\n",
        "f1_values = [0.26,0.22,0.75,0.47,0.64,0.70,0.52]\n"
      ],
      "metadata": {
        "id": "UuCHHu9I1Lmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have loaded your DataFrame 'df' with the columns 'label_code' and 'predicted_label_code'\n",
        "y_true = df['label_code']\n",
        "# y_pred = df['prediction_negex']\n",
        "y_pred = df['predicted']\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate additional evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Append metric values to the respective lists\n",
        "accuracy_values.append(accuracy)\n",
        "precision_values.append(precision)\n",
        "recall_values.append(recall)\n",
        "f1_values.append(f1)\n",
        "\n",
        "print(accuracy, precision, recall, f1)"
      ],
      "metadata": {
        "id": "fse5OQWiBtri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a box plot for the accumulated evaluation metrics\n",
        "metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "metric_values = [accuracy_values, precision_values, recall_values, f1_values]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=metric_values, orient=\"v\", palette=\"Set3\")\n",
        "sns.stripplot(data=metric_values, orient=\"v\", palette=\"Set1\", size=4, jitter=True)\n",
        "\n",
        "# Add legends\n",
        "legends = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=5, label='Data Points')]\n",
        "plt.legend(handles=legends, loc='upper right')\n",
        "plt.ylim(0, 1)  # Set the y-axis range from 0 to 1\n",
        "\n",
        "plt.xticks(range(4), metric_names)\n",
        "plt.ylabel(\"Metric Value\")\n",
        "plt.title(\"Box Plots for Accumulated Evaluation Metrics with Data Points\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PlguWUnHbY9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "df.to_excel(\"dataset_gb.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "3uTPjWzZevvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1viUzV69A4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVdveMEJ9BWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}