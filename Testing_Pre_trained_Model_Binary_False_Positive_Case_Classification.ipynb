{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGq5vYP6E2CnYrkfGpKTJf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahidhamidi/ML-Project/blob/main/Testing_Pre_trained_Model_Binary_False_Positive_Case_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "FsX473Nv-cdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJTmRVfsfmn8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import re\n",
        "import spacy\n",
        "from gensim import corpora, models\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4jks0qB537Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Clear the TensorFlow session and reset the computational graph\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "v5c3VdvZCIr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the CSV file\n",
        "csv_file_path = '/content/true_positive_experimentation.csv'\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract the text data from the 'text' column\n",
        "text_data = df['doc_text_original'].tolist()"
      ],
      "metadata": {
        "id": "BxQD0Mprfoi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P_abM3qjAFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        # Handle non-string elements here, for example, by returning an empty string\n",
        "        return \"\"\n",
        "\n",
        "    # Replace contractions\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "\n",
        "    # Remove special characters using regular expressions\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        # If the token is in title case (first letter uppercase, rest lowercase),\n",
        "        # convert it to lowercase and keep only the first letter.\n",
        "        if token.text.istitle():\n",
        "            tokens.append(token.text[0].lower())\n",
        "        else:\n",
        "            tokens.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply preprocessing to the DataFrame, including removing NaN values\n",
        "df['preprocessed_data'] = df['doc_text_original'].apply(preprocess_text)\n",
        "\n",
        "# Filter out rows with empty strings (resulting from non-string elements)\n",
        "df = df[df['preprocessed_data'] != \"\"]\n",
        "\n",
        "# Filter out non-string elements from text_data and preprocess the rest\n",
        "preprocessed_data = [preprocess_text(text) for text in text_data if isinstance(text, str)]"
      ],
      "metadata": {
        "id": "MI2PkTrzmq9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of words per list item\n",
        "word_counts = [len(text.split()) for text in preprocessed_data]\n",
        "\n",
        "# Calculate the mean word count\n",
        "mean_word_count = sum(word_counts) / len(word_counts)\n",
        "\n",
        "# Create a histogram to visualize the distribution of word counts\n",
        "plt.hist(word_counts, bins=range(1, max(word_counts) + 2), alpha=0.5, edgecolor='black')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Word Count Distribution')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add a vertical dashed red line for the mean\n",
        "plt.axvline(x=mean_word_count, color='red', linestyle='--', label=f'Mean Word Count ({mean_word_count:.2f})')\n",
        "\n",
        "# Display a legend\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9BaoLz9Y6O_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with pre-trained model\n",
        "\n",
        "# Load the pre-trained model from TensorFlow Hub\n",
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "# embedding = \"https://tfhub.dev/google/nnlm-en-dim128/2\"\n",
        "# embedding = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=False)\n",
        "\n",
        "# Flatten and join the tokenized data into a single string for each document\n",
        "X_test = [\" \".join(doc) for doc in preprocessed_data]\n",
        "\n",
        "# Define the number of models to average\n",
        "num_models = 5  # You can adjust this based on your preference\n",
        "\n",
        "# Create a list to store the models\n",
        "models = []\n",
        "\n",
        "# Train and store multiple models\n",
        "for i in range(num_models):\n",
        "    model = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with binary cross-entropy loss and metric(s) of your choice\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model on your data (replace with your actual training code)\n",
        "    # model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
        "\n",
        "    models.append(model)  # Store the trained model\n",
        "\n",
        "# Make predictions using each model\n",
        "predictions = []\n",
        "for model in models:\n",
        "    predictions.append(model.predict(X_test))\n",
        "\n",
        "# Average the predictions\n",
        "average_predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "# Assuming average_predictions contains probability scores, you can threshold them to get binary predictions\n",
        "binary_predictions = (average_predictions > 0.5).astype(np.int32)"
      ],
      "metadata": {
        "id": "HBEqtXqzngCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'binary_predictions' contains your binary predictions\n",
        "# Ensure that the length of 'binary_predictions' matches the number of rows you want to update\n",
        "num_rows_to_update = len(binary_predictions)\n",
        "\n",
        "# Update the \"predicted_label_code\" column with the binary predictions for the first 'num_rows_to_update' rows\n",
        "df.loc[:num_rows_to_update - 1, 'predicted_label_code'] = binary_predictions\n",
        "\n",
        "# Now, 'df' should have the \"predicted_label_code\" column filled with the predictions for the specified number of rows\n"
      ],
      "metadata": {
        "id": "NZSWA-VTniCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have loaded your DataFrame 'df' with the columns 'label_code' and 'predicted_label_code'\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = (df['label_code'] == df['predicted_label_code']).sum()\n",
        "total_predictions = len(preprocessed_data)\n",
        "accuracy = (correct_predictions / total_predictions) * 100\n",
        "\n",
        "print(f\"Number of correct predictions: {correct_predictions}\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "uaJyv_WUo-wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have loaded your DataFrame 'df' with the columns 'label_code' and 'predicted_label_code'\n",
        "y_true = df['label_code']\n",
        "y_pred = df['predicted_label_code']\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Calculate additional evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# Create a heatmap of the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "ZuGEHBNKuJfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have loaded your DataFrame 'df' with the columns 'label_code' and 'predicted_label_code'\n",
        "y_true = df['label_code']\n",
        "y_score = df['predicted_label_code']  # This should contain the predicted probabilities or scores\n",
        "\n",
        "# Calculate the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "\n",
        "# Calculate the AUC (Area Under the Curve)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mg_2Hqp1uKFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTYLXnfUAno7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have your DataFrame 'df' with the updates\n",
        "\n",
        "# Specify the file path where you want to save the CSV file\n",
        "output_csv_file = \"true_positive_updated.csv\"\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to {output_csv_file}\")\n"
      ],
      "metadata": {
        "id": "tg6YW5iUv1gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smiQ-ESnwMjB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}