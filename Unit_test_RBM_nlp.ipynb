{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwC9uU+w1vPcVPLMuONwBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahidhamidi/ML-Project/blob/main/Unit_test_RBM_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "from tqdm.auto import tqdm\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTtKeUGkRzhI",
        "outputId": "f63fa77b-8eae-4b66-8e5e-e4145942807c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/sample_data/robust_test_dataset.xlsx\")"
      ],
      "metadata": {
        "id": "xqtq6m0dRx7P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "02Rj1ntGRn96"
      },
      "outputs": [],
      "source": [
        "def text_extraction(text,topics):\n",
        "\n",
        "  # Define the topics as flexible terms (subwords or whole words)\n",
        "  # topics = [\"gas shows\" , \"oil show\", \"overpressure\", \"fluid communication\" , \"gas bearing\" , \"h2s\" , \"co2\" , \"stuck pipe\" , \"pressure build up\", \"sand production\"]\n",
        "\n",
        "\n",
        "  # Replace hyphens with blank spaces in all items in the topics list\n",
        "  topics = [topic.replace('-', ' ') for topic in topics]\n",
        "\n",
        "  # Define the negation terms pattern with case-insensitivity\n",
        "  negation_terms = r'(?i)(?:(?<=\\s)|^)(no|non|unlikely|none|not|nor|without|lack|rather)(?=\\s|$|\\b)'\n",
        "\n",
        "  # Initialize a list to store selected sentences\n",
        "  selected_sentences = []\n",
        "\n",
        "  # Initialize a flag to check for both topic terms and negation terms\n",
        "  both_detected = False\n",
        "\n",
        "  # Tokenize the text into sentences using NLTK's sent_tokenize\n",
        "  sentences = sent_tokenize(text)\n",
        "\n",
        "  # Initialize flags to check for both topic terms and negation terms\n",
        "  topic_found = False\n",
        "  negation_detected = False\n",
        "\n",
        "  if len(sentences) == 1:\n",
        "    selected_sentences.append(text)\n",
        "    print(0)\n",
        "\n",
        "  else:\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "      # remove apostrophes from string\n",
        "      sentence = sentence.replace(\"'\", \"\")\n",
        "      sentence = sentence.replace(\",\", \"\")\n",
        "\n",
        "      # Tokenize the sentence into words\n",
        "      lemmatized_words = word_tokenize(sentence)\n",
        "\n",
        "      # Join the lemmatized words back into a sentence\n",
        "      lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "      # Initialize an empty pattern string\n",
        "      pattern_string = r'(?i)(?:(?<=\\s)|^)'\n",
        "\n",
        "      for topic_pattern in topics:\n",
        "        words = topic_pattern.split()\n",
        "\n",
        "        if len(words) >= 2:\n",
        "\n",
        "          # Split the topic words by spaces and hyphens and create a pattern to match them\n",
        "          # topic_pattern = re.compile(r'(?i)\\b' + r'\\b|\\b'.join(re.escape(topic) for topic in topics) + r'\\b')\n",
        "\n",
        "          # Create a regex pattern to match the multi-word topic with any characters in between\n",
        "          pattern_string = r'(?i)\\b' + r'\\s*[,.\\s]*\\s*'.join(re.escape(word) for word in words) + r'\\b'\n",
        "\n",
        "          # Compile the regex pattern\n",
        "          topic_pattern = re.compile(pattern_string, re.IGNORECASE)\n",
        "\n",
        "\n",
        "          if re.search(topic_pattern, lemmatized_sentence.lower()):\n",
        "                topic_found = True\n",
        "                break\n",
        "\n",
        "        elif len(words) != 0:\n",
        "            single_pattern = rf'(?i)(?:(?<=\\s)|^)({re.escape(words[0])})(?=\\s|$)'\n",
        "            if re.search(single_pattern, lemmatized_sentence.lower()):\n",
        "                topic_found = True\n",
        "                break\n",
        "\n",
        "      # Check if the lemmatized sentence contains negation terms\n",
        "      if re.search(negation_terms, lemmatized_sentence.lower()):\n",
        "          negation_detected = True\n",
        "      else:\n",
        "          negation_detected = False\n",
        "\n",
        "      counter = 0\n",
        "      # If a topic term is found and either no negation terms are found or negation_detected is False, append the sentence\n",
        "      if (topic_found is True) and (negation_detected is True):\n",
        "          selected_sentences.append(lemmatized_sentence)\n",
        "          both_detected = True\n",
        "          print(1)\n",
        "          break\n",
        "\n",
        "      elif (topic_found is True) and (negation_detected is False):\n",
        "          selected_sentences.append(lemmatized_sentence)\n",
        "          both_detected = False\n",
        "          counter = counter + 1\n",
        "          print(2)\n",
        "\n",
        "          if counter >1:\n",
        "            break\n",
        "          break\n",
        "\n",
        "      # else:\n",
        "      #   selected_sentences.append(sentences)\n",
        "\n",
        "\n",
        "\n",
        "    # If no sentence with both topic and negation terms is found, append the first sentence without a negation term\n",
        "    if topic_found is False:\n",
        "        selected_sentences.append(text)\n",
        "\n",
        "  # print(\"Selected Sentence: \", selected_sentences)\n",
        "  # print(\"Topic word found: \",topic_found)\n",
        "  # print(\"Negation word found: \",negation_detected)\n",
        "  return selected_sentences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Drilling shale as above with _ occasional. streaks of sand. Fair - to good gas,.show.: ' \"\n",
        "selected = text_extraction(text,\"gas show\")\n",
        "selected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srtfvbAbR4V4",
        "outputId": "ae8729fc-535e-4671-f4e8-bb8be0b5c3c7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Drilling shale as above with _ occasional. streaks of sand. Fair - to good gas,.show.: ' \"]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head()"
      ],
      "metadata": {
        "id": "elaUh_8QscWa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"gas show\"\n",
        "\n",
        "for doc in df[\"doc_text_original\"]:\n",
        "  selected = text_extraction(doc,topic)\n",
        "  print(selected)\n",
        "\n",
        "  # Add the selected_sentences as a new column named 'selected_sentence'\n",
        "  # df['selected_sentence'] = selected\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtbCxcMCav20",
        "outputId": "b15c7896-566a-4ae3-9d5d-ec03a26ffc0c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[\"'Shows: (0il, Gas, â€˜ Show of gas while drilling and circulating. '\"]\n",
            "2\n",
            "['A thin 8 foot sand in the lower portipn of the Eocene had a slight gas show .']\n",
            "0\n",
            "[\"'Shows: (0il, Gas, â€˜ Show of gas while drilling and circulating. '\"]\n",
            "['\"Drilling shale as above with _ occasional. streaks of sand. Fair - to good gas,.show.: \\' \"']\n",
            "[\"'74 | -~ | 73| Drilling shale as above with â€™ .| | occasional streaks of sand. Fair - R N ) good gas,.show.: N '\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn0aMZXRbgY0",
        "outputId": "bea06f6e-9b6c-407a-fcd4-cceab144b2fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10201"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zp85rrt9jBwW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ogHcxs6uJ84"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}