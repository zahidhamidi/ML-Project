{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVBxAzZcxZM+tJXwNWTmyg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahidhamidi/ML-Project/blob/main/Unit_test_RBM_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTtKeUGkRzhI",
        "outputId": "372f316b-e809-4c14-8619-4cb8a8babbce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/sample_data/robust_test_dataset.xlsx\")"
      ],
      "metadata": {
        "id": "xqtq6m0dRx7P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '535 mMD RKB, as they are correlated to gas bearing sands in other wells. They did not, however, display seismic anomalies at the well site. '"
      ],
      "metadata": {
        "id": "-JD4aknFR9Mk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Rj1ntGRn96",
        "outputId": "c58ae49a-b5c0-4487-d82a-2c6125be04d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Sentence:  ['535 mMD RKB as they are correlated to gas bearing sands in other wells .']\n",
            "Topic word found:  True\n",
            "Negation word found:  False\n"
          ]
        }
      ],
      "source": [
        "# Define the topics as flexible terms (subwords or whole words)\n",
        "# topics = [\"gas shows\" , \"oil show\", \"overpressure\", \"fluid communication\" , \"gas bearing\" , \"h2s\" , \"co2\" , \"stuck pipe\" , \"pressure build up\", \"sand production\"]\n",
        "topics = [\"gas-bearing\"]\n",
        "\n",
        "# Replace hyphens with blank spaces in all items in the topics list\n",
        "topics = [topic.replace('-', ' ') for topic in topics]\n",
        "\n",
        "# Define the negation terms pattern with case-insensitivity\n",
        "negation_terms = r'(?i)(?:(?<=\\s)|^)(no|non|unlikely|none|not|nor|without|lack|rather)(?=\\s|$|\\b)'\n",
        "\n",
        "# Initialize a list to store selected sentences\n",
        "selected_sentences = []\n",
        "\n",
        "# Initialize a flag to check for both topic terms and negation terms\n",
        "both_detected = False\n",
        "\n",
        "# Tokenize the text into sentences using NLTK's sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Initialize flags to check for both topic terms and negation terms\n",
        "topic_found = False\n",
        "negation_detected = False\n",
        "\n",
        "for sentence in sentences:\n",
        "\n",
        "  # remove apostrophes from string\n",
        "  sentence = sentence.replace(\"'\", \"\")\n",
        "  sentence = sentence.replace(\",\", \" \")\n",
        "\n",
        "  # Tokenize the sentence into words\n",
        "  words = word_tokenize(sentence)\n",
        "\n",
        "  # Join the lemmatized words back into a sentence\n",
        "  lemmatized_sentence = ' '.join(words)\n",
        "\n",
        "\n",
        "\n",
        "  # Initialize an empty pattern string\n",
        "  pattern_string = r'(?i)(?:(?<=\\s)|^)'\n",
        "\n",
        "  for topic_pattern in topics:\n",
        "    # words = topic_pattern.split()\n",
        "\n",
        "    if len(words) >= 2:\n",
        "\n",
        "      # Split the topic words by spaces and hyphens and create a pattern to match them\n",
        "      topic_pattern = re.compile(r'(?i)\\b' + r'\\b|\\b'.join(re.escape(topic) for topic in topics) + r'\\b')\n",
        "\n",
        "      if re.search(topic_pattern, lemmatized_sentence.lower()):\n",
        "            topic_found = True\n",
        "            break\n",
        "\n",
        "    elif len(words) != 0:\n",
        "        single_pattern = rf'(?i)(?:(?<=\\s)|^)({re.escape(words[0])})(?=\\s|$)'\n",
        "        if re.search(single_pattern, lemmatized_sentence.lower()):\n",
        "            topic_found = True\n",
        "            break\n",
        "\n",
        "  # Check if the lemmatized sentence contains negation terms\n",
        "  if re.search(negation_terms, lemmatized_sentence.lower()):\n",
        "      negation_detected = True\n",
        "  else:\n",
        "      negation_detected = False\n",
        "\n",
        "  # If a topic term is found and either no negation terms are found or negation_detected is False, append the sentence\n",
        "  if (topic_found is True) and (negation_detected is True):\n",
        "      selected_sentences.append(lemmatized_sentence)\n",
        "      both_detected = True\n",
        "      break\n",
        "\n",
        "  elif (topic_found is True) and (negation_detected is False):\n",
        "      selected_sentences.append(lemmatized_sentence)\n",
        "      both_detected = True\n",
        "      break\n",
        "\n",
        "\n",
        "  # If no sentence with both topic and negation terms is found, append the first sentence without a negation term\n",
        "  if not both_detected or topic_found is False:\n",
        "      selected_sentences.append(lemmatized_sentence)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Selected Sentence: \", selected_sentences)\n",
        "print(\"Topic word found: \",topic_found)\n",
        "print(\"Negation word found: \",negation_detected)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srtfvbAbR4V4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}